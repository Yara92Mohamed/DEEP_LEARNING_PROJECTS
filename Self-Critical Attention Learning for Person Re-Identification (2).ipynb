{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff359fb",
   "metadata": {},
   "source": [
    "### The model architecture includes five self-attention blocks, which are placed on top of the first convolution layer of the network and the output layer of each residual block. The self-attention block is defined as a combination of convolutional layers, batch normalization, sigmoid activation, element-wise multiplication, average pooling, and flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5662232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "input_shape = (256, 128, 3)\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the self-critical attention module\n",
    "attention = models.Sequential()\n",
    "attention.add(layers.Conv2D(256, kernel_size=(1, 1), strides=(1, 1), activation='relu'))\n",
    "attention.add(layers.BatchNormalization())\n",
    "attention.add(layers.Conv2D(1, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92784acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with the self-critical attention module\n",
    "inp = layers.Input(shape=input_shape)\n",
    "features = model(inp)\n",
    "att_map = attention(features)\n",
    "att_features = layers.Multiply()([features, att_map])\n",
    "att_pool = layers.AveragePooling2D(pool_size=(8, 8))(att_features)\n",
    "flatten = layers.Flatten()(att_pool)\n",
    "out = layers.Dense(2, activation='softmax')(flatten)\n",
    "model = models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62843138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=loss_fn, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd46388",
   "metadata": {},
   "source": [
    "### In the above example, we use the same CNN architecture as in the paper, followed by the self-critical attention module. We compile the model using stochastic gradient descent optimizer with a learning rate of 0.01, momentum of 0.9, and weight decay of 1e-4. We use the categorical cross-entropy loss function for classification. The regularization rate is controlled by the weight decay parameter in the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837fd8f",
   "metadata": {},
   "source": [
    "### We use the ImageDataGenerator class to perform data augmentation on the training set. We train the model for 10 epochs and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train',\n",
    "    target_size=(256, 128),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ced08",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'test',\n",
    "    target_size=(256, 128),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af52a1",
   "metadata": {},
   "source": [
    "### During training, the code randomly selects a batch of images from the training set, and feeds the images through the model to obtain feature maps and attention maps. The code then predicts the critic value for each image using the feature maps and attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74900033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf1d15e",
   "metadata": {},
   "source": [
    "### After training is complete, the trained model parameters (i.e., backbone network, attention model, and critic module) can be used for image retrieval and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aae20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate_generator(validation_generator, validation_generator.samples // validation_generator.batch_size)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6b2c4",
   "metadata": {},
   "source": [
    "### The code defines a ResNet-50 model with self-attention blocks and triplet loss for training. The ResNet-50 model is defined using the Keras functional API, and includes both classification and feature extraction outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e04c1c",
   "metadata": {},
   "source": [
    "### During training, three data augmentation methods are applied, including random cropping, horizontal flipping, and erasing. Each mini-batch consists of randomly selected P identities and randomly sampled K images for each identity from the training set to cooperate the requirement of triplet loss. Here, P is set to 24 and K is set to 4. Each input image is resized to 384 Ã— 192 for exploiting fine-grained information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db216b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "input_shape = (384, 192, 3)\n",
    "\n",
    "# Define the ResNet50 architecture\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), strides=strides, name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides, name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = layers.BatchNormalization(name=bn_name_base + '1')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b6630",
   "metadata": {},
   "source": [
    "### Yes, the code provided is a high-level algorithm that outlines the training procedure used in the paper that the code is based on. The code implements this algorithm by defining a ResNet-50 model with self-attention blocks and triplet loss, and training the model using the procedure outlined in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60446fa",
   "metadata": {},
   "source": [
    "### Specifically, the ResNet-50 model is defined using the Keras functional API, and includes both classification and feature extraction outputs. The model architecture includes five self-attention blocks, which are placed on top of the first convolution layer of the network and the output layer of each residual block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b618b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(input_tensor=None, input_shape=None, pooling=None, classes=1000):\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not tf.keras.backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(64, (7, 7), strides=(1, 1), name='conv1')(x)\n",
    "    x = layers.BatchNormalization(name='bn_conv1')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    if pooling == 'avg':\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "    elif pooling == 'max':\n",
    "        x = layers.GlobalMaxPooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the self-attention layer\n",
    "def self_attention_block(input_tensor):\n",
    "    x = layers.Conv2D(256, kernel_size=(1, 1), strides=(1, 1), activation='relu')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(1, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid')(x)\n",
    "    x = layers.Multiply()([input_tensor, x])\n",
    "    x = layers.AveragePooling2D(pool_size=(8, 4))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return x\n",
    "\n",
    "# Define the model with self-attention blocks\n",
    "x = self_attention_block(x)\n",
    "for i in range(4):\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=6+i, block='a')\n",
    "    x = self_attention_block(x)\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=6+i, block='b')\n",
    "    x = self_attention_block(x)\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=6+i, block='c')\n",
    "    x = self_attention_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad866b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output layers\n",
    "    cls_output = layers.Dense(classes, activation='softmax', name='cls_output')(x)\n",
    "    feat_output = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='feat_output')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = models.Model(img_input, [cls_output, feat_output], name='resnet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af07df",
   "metadata": {},
   "source": [
    "### The model is trained for 160 epochs using the Adam optimizer. The initial learning rate is set to 0.0004 and is divided by 10 every 40 epochs. The weight decay factor for L2 regularization is set to 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded968d0",
   "metadata": {},
   "source": [
    "### The margin parameter of triplet loss and the label smoothing regularization rate are set as 0.3 and 0.1, respectively. The weighting coefficients about loss functions {Jcls, Jtri, Jcri, Jmse} are set as {1.0, 1.0, 0.3, 1.0} respectively in all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for training\n",
    "batch_size = 96\n",
    "num_epochs = 160\n",
    "learning_rate = 0.0004\n",
    "decay_rate = learning_rate / num_epochs\n",
    "momentum = 0.9\n",
    "margin = 0.3\n",
    "smoothing_rate = 0.1\n",
    "weight_decay = 0.001\n",
    "num_identities = 24\n",
    "num_images_per_identity = 4\n",
    "loss_weights = {'cls_output': 1.0, 'feat_output': 1.0, 'center_loss': 0.3, 'mse_loss': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27761af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data augmentation methods\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cec438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train_data_directory',\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9be37",
   "metadata": {},
   "source": [
    "### The code uses the predicted features and true labels to compute the triplet loss and classification loss, and uses the attention maps to compute the center loss. The code then updates the model parameters using the gradients of these loss functions, according to the update rules outlined in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the triplet loss function\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), margin)\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=None)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ebda02",
   "metadata": {},
   "source": [
    "### The code uses the predicted features and true labels to compute the triplet loss and classification loss, and uses the attention maps to compute the center loss. The code then updates the model parameters using the gradients of these loss functions, according to the update rules outlined in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ea042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the center loss function\n",
    "def center_loss(y_true, y_pred, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Define the center loss function\n",
    "    \"\"\"\n",
    "    # Get the number of features in the input\n",
    "    n_features = y_pred.get_shape()[1]\n",
    "    \n",
    "    # Compute the centers for each class\n",
    "    centers = tf.Variable(tf.zeros([n_classes, n_features]), name='centers')\n",
    "    labels = tf.cast(y_true, dtype=tf.int32)\n",
    "    centers_batch = tf.gather(centers, labels)\n",
    "    diff = centers_batch - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty array to store the number of instances in each class\n",
    "class_counts = tf.Variable(tf.zeros([n_classes]), dtype=tf.int32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc48e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the centers and class counts using the current batch\n",
    "def update_centers():\n",
    "    centers_batch = tf.gather(centers, labels)\n",
    "    diff = centers_batch - y_pred\n",
    "    unique_labels, unique_idx, unique_counts = tf.unique_with_counts(labels)\n",
    "    appear_times = tf.gather(unique_counts, unique_idx)\n",
    "    appear_times = tf.reshape(appear_times, [-1, 1])\n",
    "    diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
    "    centers_update_op = tf.scatter_sub(centers, labels, alpha * diff)\n",
    "    count_update_op = tf.scatter_add(class_counts, labels, tf.ones_like(labels, dtype=tf.int32))\n",
    "    with tf.control_dependencies([centers_update_op, count_update_op]):\n",
    "    return tf.identity(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75948c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the center loss and update the centers for each batch\n",
    "center_diff = tf.cond(tf.equal(tf.reduce_sum(class_counts), 0), lambda: tf.zeros_like(diff), update_centers)\n",
    "center_loss = tf.reduce_mean(tf.square(center_diff))\n",
    "\n",
    "return center_loss, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904867ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
